import sys
import os
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, 
                           QPushButton, QLabel, QFileDialog, QProgressBar, QMessageBox,
                           QStyle, QStyleFactory, QHBoxLayout)
from PyQt5.QtCore import QThread, pyqtSignal, Qt, QTimer
from PyQt5.QtGui import QPalette, QColor
from auth import AuthManager, LoginDialog
import faster_whisper
import torch

# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ CUDA
print("CUDA –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU:", torch.cuda.device_count())
    print("–¢–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:", torch.cuda.current_device())
    print("–ò–º—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:", torch.cuda.get_device_name(0))

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç CUDA –Ω–∞–¥ DirectML
if torch.cuda.is_available():
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
    dml = None
else:
    # –ü—Ä–æ–±—É–µ–º DirectML —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç CUDA
    try:
        import torch_directml
        dml = torch_directml.device()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É DirectML
        test_tensor = torch.randn(1, 1).to(dml)
        del test_tensor  # –û—á–∏—â–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä
        print("DirectML —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
    except (ImportError, RuntimeError) as e:
        print(f"DirectML –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {str(e)}")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        dml = None

from deep_translator import GoogleTranslator
from TTS.api import TTS
import ffmpeg
import logging
import librosa
import soundfile as sf
from demucs.audio import AudioFile, save_audio
from demucs.pretrained import get_model
from demucs.apply import apply_model
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoProcessor(QThread):
    progress = pyqtSignal(int)
    status = pyqtSignal(str)
    finished = pyqtSignal()
    
    def __init__(self, video_path):
        super().__init__()
        self.video_path = video_path
        self.working_dir = "temp"
        os.makedirs(self.working_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Demucs
        self.separator = get_model('htdemucs')
        self.separator.eval()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        if torch.cuda.is_available():
            self.device = "cuda"
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
            # –Ø–≤–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –Ω–∞ GPU
            self.separator.cuda()
        else:
            if 'dml' in globals() and dml is not None:
                self.device = dml
                print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DirectML –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
            else:
                self.device = "cpu"
                print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π")
            
        self.separator.to(self.device)
        
    def extract_audio(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ"""
        self.status.emit("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ...")
        output_audio = os.path.join(self.working_dir, "audio.wav")
        try:
            stream = ffmpeg.input(self.video_path)
            stream = ffmpeg.output(stream, output_audio, acodec='pcm_s16le', ac=2, ar='44100')
            ffmpeg.run(stream, overwrite_output=True)
            return output_audio
        except ffmpeg.Error as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞—É–¥–∏–æ: {str(e)}")
            raise

    def separate_audio(self, audio_path):
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ –Ω–∞ –≥–æ–ª–æ—Å –∏ —Ñ–æ–Ω"""
        self.status.emit("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ –Ω–∞ –≥–æ–ª–æ—Å –∏ —Ñ–æ–Ω...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            audio_file = AudioFile(audio_path).read(streams=0, samplerate=self.separator.samplerate, channels=self.separator.audio_channels)
            ref = audio_file.mean(0)
            audio_file = (audio_file - ref.mean()) / ref.std()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å
            audio_file = audio_file.unsqueeze(0)
            
            # –î–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
            self.separator.cpu()
            audio_file = audio_file.cpu()
            
            with torch.no_grad():
                sources = apply_model(self.separator, audio_file, split=True, device="cpu")
                sources = sources * ref.std() + ref.mean()
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ DirectML/GPU
            self.separator.to(self.device)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –¥–æ—Ä–æ–∂–∫–∏
            sources = sources.numpy()
            vocals = sources[0, self.separator.sources.index('vocals')]
            no_vocals = np.zeros_like(vocals)
            
            # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ –≤–æ–∫–∞–ª–∞
            for source in ['drums', 'bass', 'other']:
                idx = self.separator.sources.index(source)
                no_vocals += sources[0, idx]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            vocals_path = os.path.join(self.working_dir, "vocals.wav")
            background_path = os.path.join(self.working_dir, "background.wav")
            
            sf.write(vocals_path, vocals.T, self.separator.samplerate)
            sf.write(background_path, no_vocals.T, self.separator.samplerate)
            
            return vocals_path, background_path
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∞—É–¥–∏–æ: {str(e)}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.separator.to(self.device)
            raise

    def get_reference_audio(self, vocals_path):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞"""
        self.status.emit("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞...")
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        y, sr = librosa.load(vocals_path, sr=24000)
        
        # –ë–µ—Ä–µ–º 10 —Å–µ–∫—É–Ω–¥ –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã –∏–ª–∏ –≤—Å—ë –∞—É–¥–∏–æ –µ—Å–ª–∏ –æ–Ω–æ –∫–æ—Ä–æ—á–µ
        duration = len(y) / sr
        if duration > 10:
            start = int((duration/2 - 5) * sr)
            end = int((duration/2 + 5) * sr)
            y = y[start:end]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π —Ñ–∞–π–ª
        reference_path = os.path.join(self.working_dir, "reference.wav")
        sf.write(reference_path, y, sr)
        return reference_path
            
    def transcribe_audio(self, vocals_path):
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é Faster Whisper –∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞"""
        self.status.emit("–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏...")
        model_size = "base"
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è Faster Whisper
        # Faster Whisper –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ 'cuda' –∏–ª–∏ 'cpu' –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
        if torch.cuda.is_available() and str(self.device) == 'cuda':
            device = "cuda"
            compute_type = "float16"
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA –¥–ª—è Whisper")
        else:
            device = "cpu"
            compute_type = "int8"
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –¥–ª—è Whisper")
            
        model = faster_whisper.WhisperModel(model_size, device=device, compute_type=compute_type)
        segments, info = model.transcribe(vocals_path, beam_size=5)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        result_text = ""
        for segment in segments:
            result_text += segment.text + " "
            
        return result_text.strip()
        
    def translate_text(self, text):
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
        self.status.emit("–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞...")
        translator = GoogleTranslator(source='auto', target='ru')
        return translator.translate(text)
        
    def generate_speech(self, text, reference_audio):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
        self.status.emit("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏...")
        output_speech = os.path.join(self.working_dir, "generated_speech.wav")
        
        # TTS –ø–æ–∫–∞ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º —Å DirectML, –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA –∏–ª–∏ CPU
        if torch.cuda.is_available() and str(self.device) == 'cuda':
            device = "cuda"
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA –¥–ª—è TTS")
        else:
            device = "cpu"
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –¥–ª—è TTS")
            
        tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
        tts.to(device)
        
        tts.tts_to_file(text=text, 
                        file_path=output_speech,
                        language="ru",
                        speaker_wav=reference_audio)
        
        return output_speech

    def mix_audio(self, speech_path, background_path):
        """–°–º–µ—à–∏–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—á–∏ —Å —Ñ–æ–Ω–æ–≤–æ–π –º—É–∑—ã–∫–æ–π"""
        self.status.emit("–ú–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª—ã
        speech, sr = librosa.load(speech_path, sr=44100)
        background, _ = librosa.load(background_path, sr=44100)
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É
        if len(speech) > len(background):
            background = np.pad(background, (0, len(speech) - len(background)))
        else:
            speech = np.pad(speech, (0, len(background) - len(speech)))
        
        # –ú–∏–∫—à–∏—Ä—É–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        mixed = speech * 0.7 + background * 0.3
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = os.path.join(self.working_dir, "final_audio.wav")
        sf.write(output_path, mixed, sr)
        return output_path
        
    def merge_audio_video(self, audio_path):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –Ω–æ–≤—ã–º –∞—É–¥–∏–æ"""
        self.status.emit("–°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ...")
        output_video = "output_video.mp4"
        try:
            input_video = ffmpeg.input(self.video_path)
            input_audio = ffmpeg.input(audio_path)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å AAC
            stream = ffmpeg.output(input_video.video, 
                                 input_audio.audio,
                                 output_video,
                                 acodec='aac',
                                 vcodec='copy',
                                 strict='experimental')
            
            ffmpeg.run(stream, overwrite_output=True)
            return output_video
        except ffmpeg.Error as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –≤–∏–¥–µ–æ: {str(e)}")
            raise
            
    def run(self):
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ
            self.progress.emit(10)
            audio_path = self.extract_audio()
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ—Å –∏ —Ñ–æ–Ω
            self.progress.emit(20)
            vocals_path, background_path = self.separate_audio(audio_path)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞ –∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞
            self.progress.emit(30)
            reference_audio = self.get_reference_audio(vocals_path)
            
            # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞
            self.progress.emit(40)
            text = self.transcribe_audio(vocals_path)
            
            # –ü–µ—Ä–µ–≤–æ–¥
            self.progress.emit(50)
            translated_text = self.translate_text(text)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏
            self.progress.emit(60)
            new_speech = self.generate_speech(translated_text, reference_audio)
            
            # –ú–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ñ–æ–Ω–æ–º
            self.progress.emit(70)
            final_audio = self.mix_audio(new_speech, background_path)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞
            self.progress.emit(90)
            output_video = self.merge_audio_video(final_audio)
            
            self.progress.emit(100)
            self.status.emit("–ì–æ—Ç–æ–≤–æ!")
            self.finished.emit()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
            self.status.emit(f"–û—à–∏–±–∫–∞: {str(e)}")

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("DeepSynch")
        self.setGeometry(100, 100, 400, 300)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.auth_manager = AuthManager()
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –≤–∏–¥–∂–µ—Ç –∏ layout
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        layout = QVBoxLayout(central_widget)
        
        # –í–µ—Ä—Ö–Ω—è—è –ø–∞–Ω–µ–ª—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ –∏ —Ç–µ–º–æ–π
        top_panel = QHBoxLayout()
        
        # –ú–µ—Ç–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
        self.user_label = QLabel("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
        top_panel.addWidget(self.user_label)
        
        # –ö–Ω–æ–ø–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ç–µ–º—ã
        self.theme_button = QPushButton("üåô")
        self.theme_button.setFixedSize(30, 30)
        self.theme_button.clicked.connect(self.toggle_theme)
        top_panel.addWidget(self.theme_button)
        
        layout.addLayout(top_panel)
        
        # –ö–Ω–æ–ø–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏/–≤—ã—Ö–æ–¥–∞
        self.auth_button = QPushButton("–í–æ–π—Ç–∏")
        self.auth_button.clicked.connect(self.toggle_auth)
        layout.addWidget(self.auth_button)
        
        # –ö–Ω–æ–ø–∫–∞ –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–∞
        self.select_button = QPushButton("–í—ã–±—Ä–∞—Ç—å –≤–∏–¥–µ–æ")
        self.select_button.clicked.connect(self.select_file)
        layout.addWidget(self.select_button)
        
        # –ú–µ—Ç–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        self.file_label = QLabel("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
        layout.addWidget(self.file_label)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞–Ω–µ–ª—å
        progress_panel = QHBoxLayout()
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        self.progress_bar = QProgressBar()
        progress_panel.addWidget(self.progress_bar)
        
        # –ö–Ω–æ–ø–∫–∞ –æ—Ç–º–µ–Ω—ã
        self.cancel_button = QPushButton("–û—Ç–º–µ–Ω–∞")
        self.cancel_button.setEnabled(False)
        self.cancel_button.clicked.connect(self.cancel_processing)
        progress_panel.addWidget(self.cancel_button)
        
        layout.addLayout(progress_panel)
        
        # –ú–µ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
        self.time_label = QLabel("–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: --:--")
        layout.addWidget(self.time_label)
        
        # –ú–µ—Ç–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
        self.status_label = QLabel("–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        layout.addWidget(self.status_label)
        
        self.video_path = None
        self.processing = False
        self.start_time = None
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_time_estimate)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã
        self.check_auth()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–≤–µ—Ç–ª—É—é —Ç–µ–º—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.is_dark_theme = False
        self.apply_theme()
        
    def toggle_theme(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ç–µ–º–Ω–æ–π –∏ —Å–≤–µ—Ç–ª–æ–π —Ç–µ–º–æ–π"""
        self.is_dark_theme = not self.is_dark_theme
        self.theme_button.setText("‚òÄÔ∏è" if self.is_dark_theme else "üåô")
        self.apply_theme()
        
    def apply_theme(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ç–µ–º—ã"""
        if self.is_dark_theme:
            app = QApplication.instance()
            app.setStyle("Fusion")
            palette = QPalette()
            palette.setColor(QPalette.Window, QColor(53, 53, 53))
            palette.setColor(QPalette.WindowText, Qt.white)
            palette.setColor(QPalette.Base, QColor(35, 35, 35))
            palette.setColor(QPalette.AlternateBase, QColor(53, 53, 53))
            palette.setColor(QPalette.ToolTipBase, Qt.white)
            palette.setColor(QPalette.ToolTipText, Qt.white)
            palette.setColor(QPalette.Text, Qt.white)
            palette.setColor(QPalette.Button, QColor(53, 53, 53))
            palette.setColor(QPalette.ButtonText, Qt.white)
            palette.setColor(QPalette.BrightText, Qt.red)
            palette.setColor(QPalette.Highlight, QColor(42, 130, 218))
            palette.setColor(QPalette.HighlightedText, Qt.black)
            app.setPalette(palette)
        else:
            app = QApplication.instance()
            app.setStyle("Fusion")
            app.setPalette(app.style().standardPalette())
            
    def update_time_estimate(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏"""
        if self.processing and hasattr(self, 'processor'):
            progress = self.progress_bar.value()
            if progress > 0:
                elapsed = (QTimer.currentTime().msecsSinceStartOfDay() - self.start_time) / 1000
                total_estimated = (elapsed * 100) / progress
                remaining = total_estimated - elapsed
                minutes = int(remaining // 60)
                seconds = int(remaining % 60)
                self.time_label.setText(f"–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: {minutes:02d}:{seconds:02d}")
                
    def cancel_processing(self):
        """–û—Ç–º–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        if hasattr(self, 'processor') and self.processing:
            self.processor.terminate()
            self.processing = False
            self.timer.stop()
            self.progress_bar.setValue(0)
            self.status_label.setText("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
            self.cancel_button.setEnabled(False)
            self.time_label.setText("–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: --:--")
            self.select_button.setEnabled(True)
            
    def start_processing(self):
        self.processor = VideoProcessor(self.video_path)
        self.processor.progress.connect(self.progress_bar.setValue)
        self.processor.status.connect(self.status_label.setText)
        self.processor.finished.connect(self.processing_finished)
        self.processing = True
        self.start_time = QTimer.currentTime().msecsSinceStartOfDay()
        self.timer.start(1000)
        self.cancel_button.setEnabled(True)
        self.select_button.setEnabled(False)
        self.processor.start()
        
    def processing_finished(self):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        self.processing = False
        self.timer.stop()
        self.cancel_button.setEnabled(False)
        self.time_label.setText("–û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: --:--")
        self.select_button.setEnabled(True)
        
    def check_auth(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        user_info = self.auth_manager.get_current_user_info()
        if user_info:
            self.user_label.setText(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_info['name']}")
            self.auth_button.setText("–í—ã–π—Ç–∏")
            self.select_button.setEnabled(True)
        else:
            self.user_label.setText("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
            self.auth_button.setText("–í–æ–π—Ç–∏")
            self.select_button.setEnabled(False)
            
    def toggle_auth(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –≤—ã—Ö–æ–¥–æ–º"""
        if self.auth_manager.current_user:
            # –í—ã—Ö–æ–¥ –∏–∑ —Å–∏—Å—Ç–µ–º—ã
            self.auth_manager.logout()
            self.check_auth()
        else:
            # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
            login_dialog = LoginDialog(self.auth_manager, self)
            if login_dialog.exec_():
                self.check_auth()
        
    def select_file(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ",
            "",
            "Video Files (*.mp4 *.avi *.mkv)"
        )
        if file_path:
            self.video_path = file_path
            self.file_label.setText(os.path.basename(file_path))
            self.start_processing()

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec_()) 